{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.1: How to choose optimal value of k in KNN?\n",
    "\n",
    "Ans: If you carry on going, you will eventually end up with the CV error beginning to go up again. This is because the larger you make 𝑘, the more smoothing takes place, and eventually you will smooth so much that you will get a model that under-fits the data rather than over-fitting it (make 𝑘 big enough and the output will be constant regardless of the attribute values). I'd extend the plot until the CV error starts to go noticably up again, just to be sure, and then pick the 𝑘 that minimizes the CV error. The bigger you make 𝑘 the smoother the decision boundary and the more simple the model, so if computational expense is not an issue, I would go for a larger value of 𝑘 than a smaller one, if the difference in their CV errors is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.3: Is there a way to determine contribution of each feature to a specific class prediction?\n",
    "\n",
    "Ans: Imagine a feature for which neither very large nor very small values predict a class, but values in some intermediate interval do. That could be water intake to predict dehydration. But water intake probably interacts with salt intake, as eating more salt allows for a greater water intake. Now you have an interaction between two non-linear features. The decision boundary meanders around your feature-space to model this non-linearity and to ask only how much one of the features influences the risk of dehydration is simply ignorant. It is not the right question.\n",
    "\n",
    "Alternative: Another, more meaningful, question you could ask is: If I didn't have this information (if I left out this feature) how much would my prediction of a given label suffer? To do this you simply leave out a feature, train a model and look at how much precision and recall drops for each of your classes. It still informs about feature importance, but it makes no assumptions about linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.4: How is feature importance calculated in decision trees?\n",
    "\n",
    "Ans: Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples.\n",
    "\n",
    "The higher the value the more important the feature.\n",
    "\n",
    "Refer:\n",
    "https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.5: How to handle bias Vs. variance in XGBoost?\n",
    "\n",
    "Ans: \n",
    "1. Increasing the number of rounds and reducing the learning rate is a possibility. Something that is \"weird\" about gradient boosting is that running it well past the point where the training error has hit zero seems to still improve the test error (as discussed here: Is Deeper Better Only When Shallow Is Good?). You can try to train your model a little bit longer on your dataset once you have set the other parameters,\n",
    "\n",
    "\n",
    "2. The depth of the trees you grow is a very good place to start. You have to note that for every one unit of depth, you double the number of leafs to be constructed. If you were to grow trees of size two instead of size 16, it would take 1/2^14 of the time! You should try growing more smaller trees. The reason why is that the depth of the tree should represent the degree of feature interaction. This may be jargon, but if your features have a degree of interaction of 3 (Roughly: A combination of 4 features is not more powerful than a combination of 3 of those feature + the fourth), then growing trees of size larger than 3 is detrimental. Two trees of depth three will have more generalization power than one tree of depth four. This is a rather complicated concept and I will not go into it right now, but you can check this collection of papers for a start. Also, note that deep trees lead to high variance!\n",
    "\n",
    "\n",
    "3. Using subsampling, known as bagging, is great to reduce variance. If your individual trees have a high variance, bagging will average the trees and the average has less variance than individual trees. If, after tuning the depth of your trees, you still encounter high variance, try to increase subsampling (that is, reduce the fraction of data used). Subsampling of the feature space also achieves this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.6: Is feature selection really needed?\n",
    "\n",
    "Ans: https://stats.stackexchange.com/questions/215154/variable-selection-for-predictive-modeling-really-needed-in-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.7: What is hughes phenomenon?\n",
    "\n",
    "Ans: Hughes Phenomenon shows that as the number of features increases, the classifier’s performance increases as well until we reach the optimal number of features. Adding more features based on the same size as the training set will then degrade the classifier’s performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.8: How would you explain PCA to a layman?\n",
    "\n",
    "Ans: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.9: What is the difference between likelihood and probability?\n",
    "\n",
    "Ans: The answer depends on whether you are dealing with discrete or continuous random variables. So, I will split my answer accordingly. I will assume that you want some technical details and not necessarily an explanation in plain English.\n",
    "\n",
    "#### Discrete Random Variables:\n",
    "\n",
    "Suppose that you have a stochastic process that takes discrete values (e.g., outcomes of tossing a coin 10 times, number of customers who arrive at a store in 10 minutes etc). In such cases, we can calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying stochastic process (e.g., probability of coin landing heads is 𝑝 and that coin tosses are independent).\n",
    "\n",
    "Denote the observed outcomes by 𝑂 and the set of parameters that describe the stochastic process as 𝜃. Thus, when we speak of probability we want to calculate 𝑃(𝑂|𝜃). In other words, given specific values for 𝜃, 𝑃(𝑂|𝜃) is the probability that we would observe the outcomes represented by 𝑂.\n",
    "\n",
    "However, when we model a real life stochastic process, we often do not know 𝜃. We simply observe 𝑂 and the goal then is to arrive at an estimate for 𝜃 that would be a plausible choice given the observed outcomes 𝑂. We know that given a value of 𝜃 the probability of observing 𝑂 is 𝑃(𝑂|𝜃). Thus, a 'natural' estimation process is to choose that value of 𝜃 that would maximize the probability that we would actually observe 𝑂. In other words, we find the parameter values 𝜃 that maximize the following function:\n",
    "\n",
    "### 𝐿(𝜃|𝑂)=𝑃(𝑂|𝜃)\n",
    "\n",
    "𝐿(𝜃|𝑂) is called the likelihood function. Notice that by definition the likelihood function is conditioned on the observed 𝑂 and that it is a function of the unknown parameters 𝜃.\n",
    "\n",
    "#### Continuous Random Variables:\n",
    "\n",
    "In the continuous case the situation is similar with one important difference. We can no longer talk about the probability that we observed 𝑂 given 𝜃 because in the continuous case 𝑃(𝑂|𝜃)=0. Without getting into technicalities, the basic idea is as follows:\n",
    "\n",
    "Denote the probability density function (pdf) associated with the outcomes 𝑂 as: 𝑓(𝑂|𝜃). Thus, in the continuous case we estimate 𝜃 given observed outcomes 𝑂 by maximizing the following function:\n",
    "\n",
    "𝐿(𝜃|𝑂)=𝑓(𝑂|𝜃)\n",
    "\n",
    "In this situation, we cannot technically assert that we are finding the parameter value that maximizes the probability that we observe 𝑂 as we maximize the PDF associated with the observed outcomes 𝑂."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.10: What is the intuition behind beta distribution?\n",
    "\n",
    "Ans: https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain XGBoost?\n",
    "\n",
    "Ans: http://zhanpengfang.github.io/418home.html\n",
    "\n",
    "\n",
    "https://homes.cs.washington.edu/~tqchen/data/pdf/BoostedTree.pdf\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.11: How does  One hot encoding affect decision trees?\n",
    "\n",
    "Ans: 1. By one-hot encoding a categorical variable, we are inducing sparsity into the dataset which is undesirable.\n",
    "\n",
    "2.From the splitting algorithm’s point of view, all the dummy variables are independent. If the tree decides to make a split on one of the dummy variables, the gain in purity per split is very marginal. As a result, the tree is very unlikely to select one of the dummy variables closer to the root.\n",
    "\n",
    "https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.12: How does XGBoost learn the imputations for the missing values?\n",
    "\n",
    "Ans: \n",
    "\n",
    "https://datascience.stackexchange.com/questions/15305/how-does-xgboost-learn-what-are-the-inputs-for-missing-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Q.13: Explaing log-loss/cross-entropy loss?\n",
    " \n",
    " Ans: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.14: Explaing ROC curve?\n",
    "\n",
    "Ans: Roc curve is a probability distribution used to evaluate a classfication model. It's a plot between TPR and FPR and it is generally interpretated as a trade-off between sensitivity(Recall) and specificity(1-FPR). AUC is calculated from ROC curve which is area under the curve to assess the class separability of the model. AUC close to 1 explains better class separability and vice-versa. \n",
    "\n",
    "\n",
    "In ROC curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Q.15: Define learning rate?\n",
    " \n",
    "Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.16: What is momentum w.r.t NN optimization?\n",
    "\n",
    "Momentum lets the optimization algorithm remembers its last step, and adds some proportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.17: Epoch Vs Batch Vs Iteration?\n",
    "\n",
    "Epoch: one forward pass and one backward pass of all the training examples\n",
    "\n",
    "Batch: examples processed together in one pass (forward and backward)\n",
    "\n",
    "Iteration: number of training examples / Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. 18: Explaing the difference between generative and discriminative model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.19: What is the difference between batch gradient descent and stochastic gradient descent?\n",
    "\n",
    "Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.\n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.20: Why do ensembles typically have higher scores than individual models?\n",
    "\n",
    "An ensemble is the combination of multiple models to create a single prediction. The key idea for making better predictions is that the models should make different errors. That way the errors of one model will be compensated by the right guesses of the other models and thus the score of the ensemble will be higher.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.21:  SGD?\n",
    "\n",
    "#### Benefits: \n",
    "It is computationaly cheaper than batch gradient descent and can be easily used for large datasets.Now due to these frequent updates ,parameters updates have high variance and causes the Loss function to fluctuate to different intensities. This is actually a good thing because it helps us discover new and possibly better local minima , whereas Standard Gradient Descent will only converge to the minimum of the basin as mentioned above.\n",
    "\n",
    "\n",
    "#### Disadvantage:\n",
    "\n",
    "Problem with SGD is that due to the frequent updates and fluctuations it ultimately complicates the convergence to the exact minimum and will keep overshooting due to the frequent fluctuations. Although, it has been shown that as we slowly decrease the learning rate-η, SGD shows the same convergence pattern as Standard gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the advantages of mini-batch gradient descent?\n",
    "\n",
    "1. It Reduces the variance in the parameter updates , which can ultimately lead us to a much better and stable convergence.\n",
    "\n",
    "2. Can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient.\n",
    "\n",
    "3. Commonly Mini-batch sizes Range from 50 to 256, but can vary as per the application and problem being solved.\n",
    "\n",
    "4. Mini-batch gradient descent is typically the algorithm of choice when training a neural network nowadays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q.22: What are saddle points?\n",
    "\n",
    "Points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
